{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community\n",
    "#!pip3 install langchain_openai\n",
    "#pip3 install bert_score\n",
    "#!pip3 install bert-score transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542a4ec4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-11 19:58:12.946796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "span-marker is already registered. Overwriting pipeline for task span-marker...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "from openai import AzureOpenAI\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from bert_score import score\n",
    "import pycountry\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from bert_score import score\n",
    "import evaluate\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "import nltk\n",
    "nltk.download('punkt') # Make sure to download the 'punkt' tokenizer models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf814d9",
   "metadata": {},
   "source": [
    "### Load Enviroment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4c742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n",
      "Python-dotenv could not parse statement starting at line 13\n",
      "Python-dotenv could not parse statement starting at line 14\n",
      "Python-dotenv could not parse statement starting at line 15\n",
      "Python-dotenv could not parse statement starting at line 16\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "Python-dotenv could not parse statement starting at line 19\n",
      "Python-dotenv could not parse statement starting at line 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8652e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f3e87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "#openai.api_type = \"azure\"\n",
    "#openai.api_key = os.getenv(\"api_key_azure\")\n",
    "#openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "#openai.api_version = os.getenv(\"api_version\")\n",
    "#openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "  api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "embedding_model = os.getenv(\"USER_QUERY_EMBEDDING_ENGINE\") \n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f11f01",
   "metadata": {},
   "source": [
    "<h3> helper functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b32ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../models/df_embed_EN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc569537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to make simple openAI Calls\n",
    "def callOpenAI(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd62b3",
   "metadata": {},
   "source": [
    "<h3> processing modules </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6798164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEntitiesFromQuery(user_query):\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities from the following user query: \\\"{user_query}\\\" and return output in array format.\n",
    "    \n",
    "    -Entities should be directly related to the domain or topic of interest. They should represent important concepts that contribute to the understanding of the subject matter.\n",
    "    -Each entity in the knowledge graph should be distinct and have a unique identifier. This ensures clarity and avoids ambiguity when establishing relationships between entities.\n",
    "    -You Must return output in array format e.g  ['entity1','entity2'] !!!\n",
    "    -Avoid adding new lines or breaking spaces to your output. Array should be single dimension and single line !!!\n",
    "    \"\"\"\n",
    "    entity_str = callOpenAI(prompt)   \n",
    "    return entity_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c5cf6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mentioned_countries(text):\n",
    "    countries = set()\n",
    "    \n",
    "    # Tokenize the text using regular expressions to preserve punctuation marks\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    text = ' '.join(words)  # Join the tokens back into a string\n",
    "    \n",
    "    for word in text.split():\n",
    "        try:\n",
    "            country = pycountry.countries.get(name=word) #pycountry.countries.lookup(word)\n",
    "            if country != None : \n",
    "               countries.add(country.name)\n",
    "        except LookupError:\n",
    "            pass\n",
    "    \n",
    "    return list(countries)\n",
    "\n",
    "def filter_country(user_query):\n",
    "    mentioned_countries = find_mentioned_countries(user_query)\n",
    "    # print(mentioned_countries)\n",
    "    # Check if mentioned_countries is not empty\n",
    "    if mentioned_countries:\n",
    "        country = mentioned_countries[0]\n",
    "        return df[df['Country Name'] == country]\n",
    "    else:\n",
    "        # Handle the case where no countries were mentioned\n",
    "        return None  # Or return an empty DataFrame or any other suitable value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b85bd554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(user_query):\n",
    "    df_filtered = filter_country(user_query) if filter_country(user_query) is not None else None\n",
    "    \n",
    "    if df_filtered is not None and not df_filtered.empty:  # Check if DataFrame is not None and not empty\n",
    "        length = len(df_filtered.head())\n",
    "        filtered_embeddings_arrays = np.array(list(df_filtered['Embedding']))\n",
    "        index = faiss.IndexFlatIP(filtered_embeddings_arrays.shape[1]) \n",
    "        index.add(filtered_embeddings_arrays)\n",
    "\n",
    "        user_query_embedding = client.embeddings.create( \n",
    "                input=user_query ,model= embedding_model\n",
    "            ).data[0].embedding\n",
    "\n",
    "        k = min(5, length)\n",
    "        distances, indices = index.search(np.array([user_query_embedding]), k)\n",
    "        return df_filtered, distances, indices\n",
    "    else:\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc16e4",
   "metadata": {},
   "source": [
    "# KG module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b55cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a38712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities_with_relations(label, name):\n",
    "    \"\"\"\n",
    "    Search for nodes by label and name, including all their relationships and connected nodes.\n",
    "\n",
    "    :param label: The label of the nodes to search.\n",
    "    :param name: The name property value of the nodes to search.\n",
    "    :return: A list of dictionaries, each containing a node and its relationships and connected nodes.\n",
    "    \"\"\"\n",
    "    # Match nodes and their relationships, case-insensitive search\n",
    "    query = f\"\"\"\n",
    "    MATCH (n:`{label}`)-[r]-(m)\n",
    "    WHERE toLower(n.name) = toLower($name) OR toLower(n.acronym) = toLower($name)\n",
    "    RETURN n, collect(r) as relations, collect(m) as connectedNodes\n",
    "    \"\"\"\n",
    "    results = graph.run(query, name=name).data()\n",
    "    \n",
    "    # Construct a comprehensive view for each node with its relationships and connected nodes\n",
    "    entities_with_relations = []\n",
    "    for record in results:\n",
    "        entity_info = {\n",
    "            'node': record['n'],\n",
    "            'relationships': record['relations'],\n",
    "            'connected_nodes': record['connectedNodes']\n",
    "        }\n",
    "        entities_with_relations.append(entity_info)\n",
    "\n",
    "    return entities_with_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b554a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_entities_and_relationships(relationships):\n",
    "    \"\"\"\n",
    "    Separates entities and relationships from Neo4j query output into distinct structures,\n",
    "    ensuring no duplicate entities are included.\n",
    "\n",
    "    :param relationships: A list of Relationship objects from a Neo4j query.\n",
    "    :return: A tuple containing two lists: one for unique entities and another for relationships.\n",
    "    \"\"\"\n",
    "    # Using a dictionary to ensure unique entities based on a combination of name and category\n",
    "    entities_dict = {}\n",
    "    rels = []\n",
    "\n",
    "    for rel in relationships:\n",
    "        # Process both the source and target nodes for each relationship\n",
    "        for node in [rel.start_node, rel.end_node]:\n",
    "            # Define a unique identifier for each entity\n",
    "            entity_id = f\"{node['name']}_{node['category']}\"\n",
    "            \n",
    "            # If the entity is not already in the dictionary, add it\n",
    "            if entity_id not in entities_dict:\n",
    "                entities_dict[entity_id] = {\n",
    "                    'name': node['name'],\n",
    "                    'category': node.get('category', 'N/A'),\n",
    "                    #'summary': node.get('summary', 'N/A'),\n",
    "                    'acronym': node.get('acronym', 'N/A'),\n",
    "                }\n",
    "\n",
    "        # Add relationship information\n",
    "        rels.append({\n",
    "            'subject': rel.start_node['name'],\n",
    "            'object': rel.end_node['name'],\n",
    "            'relationship_type': rel.__class__.__name__,\n",
    "            'description': rel.get('description', 'N/A'),\n",
    "        })\n",
    "\n",
    "    # Convert the entities dictionary to a list to remove the unique identifier layer\n",
    "    entities_list = list(entities_dict.values())\n",
    "\n",
    "    return entities_list, rels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c30f34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(user_question, content):\n",
    "    system_prompt = \"You are a system that answers user questions based on excerpts from PDF documents provided for context. Only answer if the answer can be found in the provided context. Do not make up the answer; if you cannot find the answer, say so.\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "        {'role': 'user', 'content': content},\n",
    "    ]\n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0.2,\n",
    "                    messages=messages\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27928260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_structure(qs):\n",
    "    result_dict = {}\n",
    "\n",
    "    # Extract the DataFrame from the tuple\n",
    "    dataframe = qs[0]\n",
    "\n",
    "    # Counter to limit the loop to 10 iterations\n",
    "    count = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Define a unique identifier for each document, you can customize this based on your data\n",
    "        document_id = f\"doc-{index + 1}\"\n",
    "        # Handle NaN in content by using fillna\n",
    "        content = row[\"Content\"]\n",
    "        content = ' '.join(row[\"Content\"].split()[:160])\n",
    "        # Create a dictionary for each document\n",
    "        document_info = {\n",
    "            \"title\": row[\"Document Title\"],\n",
    "            \"extract\": content or \"\",  # You may need to adjust this based on your column names\n",
    "            \"category\": row[\"Category\"],\n",
    "            \"link\": row[\"Link\"],\n",
    "            \"thumbnail\": ''\n",
    "        }\n",
    "        # print(document_info)\n",
    "        # Add the document to the result dictionary\n",
    "        result_dict[document_id] = document_info\n",
    "\n",
    "        # Increment the counter\n",
    "        count += 1\n",
    "\n",
    "        # # Break out of the loop if the counter reaches top 10\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db75b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module to get data for specific indicators which are identified is relevant to the user query\n",
    "\n",
    "def indicatorsModule(user_query): #lower priority\n",
    "    \n",
    "    # find relevant indicators based on uesr query and extract values\n",
    "    indicators_dict={\n",
    "        \"indicator-id-1\":\"value from indicator-id-1\",\n",
    "        \"indicator-id-2\":\"value from indicator-id-2\"\n",
    "    }#temp\n",
    "    \n",
    "    return indicators_dict\n",
    "\n",
    "## module to extract text from documents and return the text and document codes\n",
    "def semanticSearchModule(user_query):\n",
    "    qs = search_embeddings(user_query) #df, distances, indices\n",
    "    # if qs != None :\n",
    "    if qs[0] is not None:\n",
    "        result_structure = map_to_structure(qs)\n",
    "        return result_structure\n",
    "    else : \n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7113366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to synthesize answer using retreival augmented generation approach\n",
    "\n",
    "def synthesisModule(user_query, excerpts_dict, indicators_dict):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, use the provided excerpts, and Sources to\n",
    "    provide the correct answer to the user's query\n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {excerpts_dict}\n",
    "    \n",
    "\n",
    "    - Answer must be coherent, relevant and should contain important details. \n",
    "    \n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=callOpenAI(llm_instructions)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "## to test this, run the full pipeline with the handleApiCall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae918347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relations(user_query, relationships):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, identify the relations that are relevant to the answering the query. \n",
    "    Delete the rest of relations and return the set of relevant relations in JSON format\n",
    "\n",
    "    \n",
    "    User Query: {user_query}\n",
    "        \n",
    "    Relations: {relationships}\n",
    "\n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=callOpenAI(llm_instructions)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16f02e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to synthesize answer using retreival augmented generation approach\n",
    "\n",
    "def synthesisModule_with_KG(user_query, entities, relationships, excerpts_dict, indicators_dict):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    \n",
    "    For the given user query, use the provided sources and relationships to provide the correct answer. \n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {excerpts_dict}\n",
    "    \n",
    "     Relationships:  {relationships}\n",
    "\n",
    "    - Give a coherent, detailed and relevant answer. \n",
    "\n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=callOpenAI(llm_instructions)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "## to test this, run the full pipeline with the handleApiCall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c57e339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleApiCall(user_query):\n",
    "        \n",
    "    ##run processing modules (in parallel)\n",
    "    #entities_dict=knowledgeGraphModule(user_query)\n",
    "    excerpts_dict=semanticSearchModule(user_query)\n",
    "    indicators_dict=indicatorsModule(user_query) ##lower priority\n",
    "    #query_idea_list=queryIdeationModule(user_query) ##lower priority\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer=synthesisModule(user_query, excerpts_dict, indicators_dict)\n",
    "    \n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":excerpts_dict\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b67bea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_api_call_with_kg(user_query, entities, relationships):\n",
    "        \n",
    "    ##run processing modules (in parallel)\n",
    "    #entities_dict=knowledgeGraphModule(user_query)\n",
    "    excerpts_dict=semanticSearchModule(user_query)\n",
    "    indicators_dict=indicatorsModule(user_query) ##lower priority\n",
    "    #query_idea_list=queryIdeationModule(user_query) ##lower priority\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer=synthesisModule_with_KG(user_query, entities, relationships, excerpts_dict, indicators_dict)\n",
    "    \n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":excerpts_dict,\n",
    "        \"entities\": entities,\n",
    "        \"relations\":relationships\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ab094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries.json\") as f:\n",
    "    data = f.read()\n",
    "    query_list = ast.literal_eval(data)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfa6fc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a28d9f",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b2a3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What strategies is UNDP implementing to support Angola's Bureau of External Relations and Advocacy?\n",
      "24\n",
      "17\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to test_outputs/test_query11_answer.json\n",
      "KG-LLM Response saved to test_outputs/Query11_answer.json\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full pipeline with retreival, synthesis of answer to user query, and structure results into api response\n",
    "\n",
    "for index in range(10,11):\n",
    "    test_query = query_list[index]\n",
    "    print (test_query)\n",
    "\n",
    "    entity_str = extractEntitiesFromQuery(test_query)\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    entity_list = ast.literal_eval(entity_str)\n",
    "\n",
    "    relations_list = []\n",
    "    for item in entity_list:\n",
    "        entities_with_relations = search_entities_with_relations('Entity', item)\n",
    "\n",
    "        for entity_info in entities_with_relations:\n",
    "            relations_list.extend(entity_info['relationships'])\n",
    "            #print(\"Connected Nodes:\", entity_info['connected_nodes'])\n",
    "\n",
    "    # Assuming 'relationships' is your list of Relationship objects from the query\n",
    "    entities, relationships = separate_entities_and_relationships(relations_list)\n",
    "    \n",
    "    filtered_relations = json.loads(filter_relations(test_query,relationships))\n",
    "\n",
    "    print (len(relationships))\n",
    "    print (len(entities))\n",
    "    \n",
    "    if len(entities) > 2:\n",
    "        final_entities = entities[:2]\n",
    "    else:\n",
    "        final_entities = entities\n",
    "    \n",
    "    if len(filtered_relations) > 20:\n",
    "        final_relationships = filtered_relations[:20]\n",
    "    else:\n",
    "        final_relationships = filtered_relations\n",
    "    \n",
    "    indicators_dict=indicatorsModule(test_query)\n",
    "    print(indicators_dict)\n",
    "    \n",
    "    \n",
    "    #test usage\n",
    "    excerpts_dict=semanticSearchModule(test_query)\n",
    "    # print(excerpts_dict)\n",
    "\n",
    "\n",
    "    #Return top 10-20 most related \n",
    "    # Define the filename to save the JSON data -  can remove later\n",
    "    json_filename = \"outputs/excerpts.json\"\n",
    "\n",
    "    # Save excerpts_dict to the JSON file just for a better preview\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(excerpts_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Excerpts saved to {json_filename}\")\n",
    "\n",
    "\n",
    "    # test usage\n",
    "    llm_response = handleApiCall(test_query) \n",
    "    # Define the filename to save the JSON data -  can remove later\n",
    "    json_filename = \"test_outputs/test_query\"+ str(index+1) + \"_answer.json\"\n",
    "\n",
    "    # Save excerpts_dict to the JSON file just for a better preview\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(llm_response, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"LLM Response saved to {json_filename}\")\n",
    "\n",
    "\n",
    "# test usage\n",
    "kg_llm_response=handle_api_call_with_kg(test_query, final_entities, final_relationships)\n",
    "\n",
    "# Define the filename to save the JSON data -  can remove later\n",
    "json_filename = \"test_outputs/Query\"+ str(index+1) + \"_answer.json\"\n",
    "\n",
    "# Save excerpts_dict to the JSON file just for a better preview\n",
    "with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(kg_llm_response, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"KG-LLM Response saved to {json_filename}\")\n",
    "print (\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7cc6f",
   "metadata": {},
   "source": [
    "<h3>testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ecee81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next step, develop automated testing for all modules\n",
    "## iterate through test_queries and build automated tests to score results\n",
    "\n",
    "# open testing dataset with queries and expected results\n",
    "\n",
    "json_filename = \"auto_eval/test_data.json\"\n",
    "\n",
    "\n",
    "with open(json_filename, 'r') as file:\n",
    "    data = file.read()\n",
    "    test_data = json.loads(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d3fd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_score_list = []\n",
    "kg_score_list = []\n",
    "combined_score_list = []\n",
    "\n",
    "for i in test_data:\n",
    "    obj = {}\n",
    "    \n",
    "    obj['id'] = i['id'] \n",
    "    obj['query'] = i['query']\n",
    "    \n",
    "    llm_response = i['llm_response']\n",
    "    kg_response = i['llm_kg_response']\n",
    "    answer = i['reference']\n",
    "    \n",
    "    reference_texts = [answer]\n",
    "    \n",
    "    # Tokenize the texts    \n",
    "    references = [word_tokenize(ref) for ref in reference_texts]\n",
    "    llm_candidate = word_tokenize(llm_response)\n",
    "    kg_candidate = word_tokenize(kg_response)\n",
    "    \n",
    "    \n",
    "    # Calculate METEOR score\n",
    "    obj['llm_score'] = meteor_score(references, llm_candidate)\n",
    "    obj['kg_score'] = meteor_score(references, kg_candidate)\n",
    "    \n",
    "    llm_score_list.append(obj['llm_score'])\n",
    "    kg_score_list.append(obj['kg_score'])\n",
    "    combined_score_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e7e211f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = {}\n",
    "obj2['LLM Meteor scores'] = llm_score_list\n",
    "obj2['KG_LLM Meteor scores'] = kg_score_list\n",
    "\n",
    "combined_score_list.append(obj2 )\n",
    "\n",
    "json_filename = \"auto_eval/meteor_score.json\"\n",
    "\n",
    "# Save excerpts_dict to the JSON file just for a better preview\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(combined_score_list, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d81ed3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kg_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f671d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_precision_list = []\n",
    "llm_recall_list = []\n",
    "llm_f1_list = []\n",
    "kg_precision_list = []\n",
    "kg_recall_list = []\n",
    "kg_f1_list = []\n",
    "\n",
    "bert_score_list = []\n",
    "\n",
    "for i in test_data:\n",
    "    obj = {}\n",
    "    \n",
    "    obj['id'] = i['id'] \n",
    "    obj['query'] = i['query']\n",
    "    \n",
    "    llm_response = i['llm_response']\n",
    "    kg_response = i['llm_kg_response']\n",
    "    answer = i['reference']\n",
    "    \n",
    "    llm_obj = {}\n",
    "    kg_obj = {}\n",
    "    # Tokenize the texts    \n",
    "    \n",
    "    reference = [answer]\n",
    "\n",
    "    llm_candidate = [llm_response]\n",
    "    kg_candidate = [kg_response]\n",
    "    \n",
    "    \n",
    "    # Calculate BERT score\n",
    "   \n",
    "    results = bertscore.compute(predictions=llm_candidate, references=reference, \n",
    "                                model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "    llm_precision_list.append(results['precision'][0])\n",
    "    llm_recall_list.append(results['recall'][0])\n",
    "    llm_f1_list.append(results['f1'][0])\n",
    "    llm_obj['precision'] = results['precision'][0]\n",
    "    llm_obj['recall'] = results['recall'][0]\n",
    "    llm_obj['f1-score'] = results['f1'][0]\n",
    "    \n",
    "    results = bertscore.compute(predictions=kg_candidate, references=reference, \n",
    "                                model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "    kg_precision_list.append(results['precision'][0])\n",
    "    kg_recall_list.append(results['recall'][0])\n",
    "    kg_f1_list.append(results['f1'][0])\n",
    "    kg_obj['precision'] = results['precision'][0]\n",
    "    kg_obj['recall'] = results['recall'][0]\n",
    "    kg_obj['f1-score'] = results['f1'][0]\n",
    "    \n",
    "    obj['LLM'] = llm_obj\n",
    "    obj['KG_LLM'] = kg_obj\n",
    "    bert_score_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3cbd287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj3 = {}\n",
    "obj3['LLM Precision scores'] = llm_precision_list\n",
    "obj3['LLM Recall scores'] = llm_recall_list\n",
    "obj3['LLM F1 scores'] = llm_f1_list\n",
    "obj3['KG Precision scores'] = kg_precision_list\n",
    "obj3['KG Recall scores'] = kg_recall_list\n",
    "obj3['KG F1 scores'] = kg_f1_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "372e609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_list.append(obj3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c8b1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_file = \"auto_eval/bert_score.json\"\n",
    "\n",
    "# Save excerpts_dict to the JSON file just for a better preview\n",
    "with open(bert_score_file, 'w') as json_file:\n",
    "    json.dump(bert_score_list, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cb31a",
   "metadata": {},
   "source": [
    "  # TODO::: \n",
    "  ##### Add citation prompt to the synthesis module. -done \n",
    "  ##### Convert notebook to flask API script. main.py - done\n",
    "  ##### Refactor PDF -> txt pipeline \n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
