{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community\n",
    "#!pip3 install langchain_openai\n",
    "#pip3 install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542a4ec4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "from openai import AzureOpenAI\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from bert_score import score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf814d9",
   "metadata": {},
   "source": [
    "### Load Enviroment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4c742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n",
      "Python-dotenv could not parse statement starting at line 13\n",
      "Python-dotenv could not parse statement starting at line 14\n",
      "Python-dotenv could not parse statement starting at line 15\n",
      "Python-dotenv could not parse statement starting at line 16\n",
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "Python-dotenv could not parse statement starting at line 19\n",
      "Python-dotenv could not parse statement starting at line 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8652e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "neo4j_pass = os.getenv(\"NEO4JPASS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3e87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "#openai.api_type = \"azure\"\n",
    "#openai.api_key = os.getenv(\"api_key_azure\")\n",
    "#openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "#openai.api_version = os.getenv(\"api_version\")\n",
    "#openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "  api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "embedding_model = os.getenv(\"USER_QUERY_EMBEDDING_ENGINE\") \n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f11f01",
   "metadata": {},
   "source": [
    "<h3> helper functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc569537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to make simple openAI Calls\n",
    "def callOpenAI(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd62b3",
   "metadata": {},
   "source": [
    "<h3> processing modules </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6798164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEntitiesFromQuery(user_query):\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities from the following user query: \\\"{user_query}\\\" and return output in array format.\n",
    "    \n",
    "    -Entities should be directly related to the domain or topic of interest. They should represent important concepts that contribute to the understanding of the subject matter.\n",
    "    -Each entity in the knowledge graph should be distinct and have a unique identifier. This ensures clarity and avoids ambiguity when establishing relationships between entities.\n",
    "    -You Must return output in array format e.g  ['entity1','entity2'] !!!\n",
    "    -Avoid adding new lines or breaking spaces to your output. Array should be single dimension and single line !!!\n",
    "    \"\"\"\n",
    "    entity_str = callOpenAI(prompt)   \n",
    "    return entity_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc16e4",
   "metadata": {},
   "source": [
    "# KG module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b55cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "graph = Graph(uri = 'bolt://localhost:7687',user='neo4j',password=neo4j_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a38712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities_with_relations(label, name):\n",
    "    \"\"\"\n",
    "    Search for nodes by label and name, including all their relationships and connected nodes.\n",
    "\n",
    "    :param label: The label of the nodes to search.\n",
    "    :param name: The name property value of the nodes to search.\n",
    "    :return: A list of dictionaries, each containing a node and its relationships and connected nodes.\n",
    "    \"\"\"\n",
    "    # Match nodes and their relationships, case-insensitive search\n",
    "    query = f\"\"\"\n",
    "    MATCH (n:`{label}`)-[r]-(m)\n",
    "    WHERE toLower(n.name) = toLower($name) OR toLower(n.acronym) = toLower($name)\n",
    "    RETURN n, collect(r) as relations, collect(m) as connectedNodes\n",
    "    \"\"\"\n",
    "    results = graph.run(query, name=name).data()\n",
    "    \n",
    "    # Construct a comprehensive view for each node with its relationships and connected nodes\n",
    "    entities_with_relations = []\n",
    "    for record in results:\n",
    "        entity_info = {\n",
    "            'node': record['n'],\n",
    "            'relationships': record['relations'],\n",
    "            'connected_nodes': record['connectedNodes']\n",
    "        }\n",
    "        entities_with_relations.append(entity_info)\n",
    "\n",
    "    return entities_with_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b554a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_entities_and_relationships(relationships):\n",
    "    \"\"\"\n",
    "    Separates entities and relationships from Neo4j query output into distinct structures,\n",
    "    ensuring no duplicate entities are included.\n",
    "\n",
    "    :param relationships: A list of Relationship objects from a Neo4j query.\n",
    "    :return: A tuple containing two lists: one for unique entities and another for relationships.\n",
    "    \"\"\"\n",
    "    # Using a dictionary to ensure unique entities based on a combination of name and category\n",
    "    entities_dict = {}\n",
    "    rels = []\n",
    "\n",
    "    for rel in relationships:\n",
    "        # Process both the source and target nodes for each relationship\n",
    "        for node in [rel.start_node, rel.end_node]:\n",
    "            # Define a unique identifier for each entity\n",
    "            entity_id = f\"{node['name']}_{node['category']}\"\n",
    "            \n",
    "            # If the entity is not already in the dictionary, add it\n",
    "            if entity_id not in entities_dict:\n",
    "                entities_dict[entity_id] = {\n",
    "                    'name': node['name'],\n",
    "                    'category': node.get('category', 'N/A'),\n",
    "                    #'summary': node.get('summary', 'N/A'),\n",
    "                    'acronym': node.get('acronym', 'N/A'),\n",
    "                }\n",
    "\n",
    "        # Add relationship information\n",
    "        rels.append({\n",
    "            'subject': rel.start_node['name'],\n",
    "            'object': rel.end_node['name'],\n",
    "            'relationship_type': rel.__class__.__name__,\n",
    "            'description': rel.get('description', 'N/A'),\n",
    "        })\n",
    "\n",
    "    # Convert the entities dictionary to a list to remove the unique identifier layer\n",
    "    entities_list = list(entities_dict.values())\n",
    "\n",
    "    return entities_list, rels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85bd554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(user_query):\n",
    "    df = pd.read_pickle('../models/df_embed_EN.pkl')\n",
    "    df_filtered = df\n",
    "    length = len(df_filtered.head())\n",
    "    filtered_embeddings_arrays = np.array(list(df_filtered['Embedding']))\n",
    "    index = faiss.IndexFlatIP(filtered_embeddings_arrays.shape[1]) \n",
    "    index.add(filtered_embeddings_arrays)\n",
    "    \n",
    "    user_query_embedding = client.embeddings.create( \n",
    "        input=user_query ,model= embedding_model\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    k = min(5, length)\n",
    "    distances, indices = index.search(np.array([user_query_embedding]), k)\n",
    "    return df_filtered, distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30f34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(user_question, content):\n",
    "    system_prompt = \"You are a system that answers user questions based on excerpts from PDF documents provided for context. Only answer if the answer can be found in the provided context. Do not make up the answer; if you cannot find the answer, say so.\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "        {'role': 'user', 'content': content},\n",
    "    ]\n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0.2,\n",
    "                    messages=messages\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27928260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_structure(qs):\n",
    "    result_dict = {}\n",
    "\n",
    "    # Extract the DataFrame from the tuple\n",
    "    dataframe = qs[0]\n",
    "\n",
    "    # Counter to limit the loop to 10 iterations\n",
    "    count = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Define a unique identifier for each document, you can customize this based on your data\n",
    "        document_id = f\"doc-{index + 1}\"\n",
    "        # Handle NaN in content by using fillna\n",
    "        content = row[\"Content\"]\n",
    "        content = ' '.join(row[\"Content\"].split()[:160])\n",
    "        # Create a dictionary for each document\n",
    "        document_info = {\n",
    "            \"title\": row[\"Document Title\"],\n",
    "            \"extract\": content or \"\",  # You may need to adjust this based on your column names\n",
    "            \"category\": row[\"Category\"],\n",
    "            \"link\": row[\"Link\"],\n",
    "            \"thumbnail\": ''\n",
    "        }\n",
    "        # print(document_info)\n",
    "        # Add the document to the result dictionary\n",
    "        result_dict[document_id] = document_info\n",
    "\n",
    "        # Increment the counter\n",
    "        count += 1\n",
    "\n",
    "        # # Break out of the loop if the counter reaches top 10\n",
    "        if count == 10:\n",
    "            break\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db75b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module to get data for specific indicators which are identified is relevant to the user query\n",
    "\n",
    "def indicatorsModule(user_query): #lower priority\n",
    "    \n",
    "    # find relevant indicators based on uesr query and extract values\n",
    "    indicators_dict={\n",
    "        \"indicator-id-1\":\"value from indicator-id-1\",\n",
    "        \"indicator-id-2\":\"value from indicator-id-2\"\n",
    "    }#temp\n",
    "    \n",
    "    return indicators_dict\n",
    "\n",
    "## module to extract text from documents and return the text and document codes\n",
    "\n",
    "def semanticSearchModule(user_query):\n",
    "    qs = search_embeddings(user_query) #df, distances, indices\n",
    "    result_structure = map_to_structure(qs)\n",
    "    return result_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7113366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to synthesize answer using retreival augmented generation approach\n",
    "\n",
    "def synthesisModule(user_query, excerpts_dict, indicators_dict):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, use the provided excerpts, and Sources to\n",
    "    provide the correct answer to the user's query\n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {excerpts_dict}\n",
    "    \n",
    "\n",
    "    - Answer must be coherent, relevant and should contain important details. \n",
    "    \n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=callOpenAI(llm_instructions)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "## to test this, run the full pipeline with the handleApiCall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16f02e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to synthesize answer using retreival augmented generation approach\n",
    "\n",
    "def synthesisModule_with_KG(user_query, entities, relationships, excerpts_dict, indicators_dict):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, use the provided excerpts, Sources, entities and relations to\n",
    "    provide the correct answer to the user's query\n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {excerpts_dict}\n",
    "    \n",
    "    Entity and Relation info: {entities} {relationships}\n",
    "\n",
    "    - Answer must be coherent, relevant and should contain important details. \n",
    "\n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=callOpenAI(llm_instructions)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "## to test this, run the full pipeline with the handleApiCall function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dfd63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleApiCall(user_query):\n",
    "        \n",
    "    ##run processing modules (in parallel)\n",
    "    #entities_dict=knowledgeGraphModule(user_query)\n",
    "    excerpts_dict=semanticSearchModule(user_query)\n",
    "    indicators_dict=indicatorsModule(user_query) ##lower priority\n",
    "    #query_idea_list=queryIdeationModule(user_query) ##lower priority\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer=synthesisModule(user_query, excerpts_dict, indicators_dict)\n",
    "    \n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":excerpts_dict\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ebbbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_api_call_with_kg(user_query):\n",
    "        \n",
    "    ##run processing modules (in parallel)\n",
    "    #entities_dict=knowledgeGraphModule(user_query)\n",
    "    excerpts_dict=semanticSearchModule(user_query)\n",
    "    indicators_dict=indicatorsModule(user_query) ##lower priority\n",
    "    #query_idea_list=queryIdeationModule(user_query) ##lower priority\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer=synthesisModule_with_KG(user_query, entities[:10], relationships[:100], excerpts_dict, indicators_dict)\n",
    "    \n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":excerpts_dict,\n",
    "        \"entities\": entities,\n",
    "        \"relations\":relationships\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d29ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries.json\") as f:\n",
    "    data = f.read()\n",
    "    query_list = ast.literal_eval(data)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84dd7eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c557c",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b2a3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What strategies is UNDP implementing to support Angola's Bureau of External Relations and Advocacy?\n",
      "24\n",
      "17\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query11_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query11_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How is Angola addressing the Sustainable Development Goals with a focus on poverty eradication?\n",
      "9\n",
      "5\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query12_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query12_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does the Ministry of Environment's support for the Paris Agreement on Climate Change manifest in Argentina's environmental policies?\n",
      "7\n",
      "6\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query13_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query13_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What is the role of Paris Agreement in sustainable energy development?\n",
      "2\n",
      "2\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query14_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query14_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does the National Social Policy Coordination Council contribute to human rights in Argentina?\n",
      "2\n",
      "4\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query15_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query15_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does the National Social Policy Coordination Council contribute to human rights?\n",
      "1\n",
      "2\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query16_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query16_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What are the key railway upgrade plans for Colombo and its suburbs?\n",
      "4\n",
      "5\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query17_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query17_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does the Ceylon Petroleum Corporation contribute to the Results Delivery Framework?\n",
      "12\n",
      "2\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query18_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query18_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What are North Macedonia's strategic priorities in partnership with the United Nations Sustainable Development Cooperation Framework for 2021-2025?\n",
      "6\n",
      "6\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query19_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query19_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What role does innovation play in UNDP's program for North Macedonia?\n",
      "26\n",
      "16\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query20_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query20_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What percentage of Niger's population has access to electricity for productive use?\n",
      "52\n",
      "39\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query21_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query21_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does the UNDP support the resilience of communities in the Diffa region of Niger?\n",
      "73\n",
      "53\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query22_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query22_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What are the main goals of Niger's Vision 2035?\n",
      "52\n",
      "39\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query23_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query23_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What are the national development priorities of Niger for 2022-2026?\n",
      "52\n",
      "39\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query24_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query24_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "What are the primary development financing challenges in Nigeria?\n",
      "32\n",
      "28\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query25_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query25_answer.json\n",
      "\n",
      "--------------\n",
      "\n",
      "How does Nigeria's poverty rate according to the World Bank in 2020 reflect on its economic policies and challenges?\n",
      "43\n",
      "34\n",
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n",
      "Excerpts saved to outputs/excerpts.json\n",
      "LLM Response saved to outputs/query26_answer.json\n",
      "KG-LLM Response saved to outputs_KG/Query26_answer.json\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full pipeline with retreival, synthesis of answer to user query, and structure results into api response\n",
    "\n",
    "for index in range(10,30):\n",
    "    test_query = query_list[index]\n",
    "    print (test_query)\n",
    "\n",
    "    entity_str = extractEntitiesFromQuery(test_query)\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    entity_list = ast.literal_eval(entity_str)\n",
    "\n",
    "    relations_list = []\n",
    "    for item in entity_list:\n",
    "        entities_with_relations = search_entities_with_relations('Entity', item)\n",
    "\n",
    "        for entity_info in entities_with_relations:\n",
    "            relations_list.extend(entity_info['relationships'])\n",
    "            #print(\"Connected Nodes:\", entity_info['connected_nodes'])\n",
    "\n",
    "    # Assuming 'relationships' is your list of Relationship objects from the query\n",
    "    entities, relationships = separate_entities_and_relationships(relations_list)\n",
    "\n",
    "    print (len(relationships))\n",
    "    print (len(entities))\n",
    "\n",
    "\n",
    "    indicators_dict=indicatorsModule(test_query)\n",
    "    print(indicators_dict)\n",
    "\n",
    "    #test usage\n",
    "    excerpts_dict=semanticSearchModule(test_query)\n",
    "    # print(excerpts_dict)\n",
    "\n",
    "\n",
    "    #Return top 10-20 most related \n",
    "    # Define the filename to save the JSON data -  can remove later\n",
    "    json_filename = \"outputs/excerpts.json\"\n",
    "\n",
    "    # Save excerpts_dict to the JSON file just for a better preview\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(excerpts_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Excerpts saved to {json_filename}\")\n",
    "\n",
    "\n",
    "    # test usage\n",
    "    llm_response = handleApiCall(test_query) \n",
    "    # Define the filename to save the JSON data -  can remove later\n",
    "    json_filename = \"outputs/query\"+ str(index+1) + \"_answer.json\"\n",
    "\n",
    "    # Save excerpts_dict to the JSON file just for a better preview\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(llm_response, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"LLM Response saved to {json_filename}\")\n",
    "\n",
    "\n",
    "    # test usage\n",
    "    kg_llm_response=handle_api_call_with_kg(test_query) \n",
    "    # Define the filename to save the JSON data -  can remove later\n",
    "    json_filename = \"outputs_KG/Query\"+ str(index+1) + \"_answer.json\"\n",
    "\n",
    "    # Save excerpts_dict to the JSON file just for a better preview\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(kg_llm_response, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"KG-LLM Response saved to {json_filename}\")\n",
    "    print (\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7cc6f",
   "metadata": {},
   "source": [
    "<h3>testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next step, develop automated testing for all modules\n",
    "## iterate through test_queries and build automated tests to score results\n",
    "\n",
    "# open testing dataset with queries and expected results\n",
    "test_queries_df=pd.read_csv(\"../testing/energy_ai_test_dataset_v0.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a737981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "nltk.download('punkt') # Make sure to download the 'punkt' tokenizer models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Example texts\n",
    "reference_texts = [\"The Gender Development Index is relevant to UNDP's work in Albania by highlighting the country's progress in gender equality, with a GDI of 0.967, informing UNDP's focus on gender equality and women's empowerment initiatives.\"]\n",
    "candidate_text = \"The Gender Development Index (GDI) is relevant to UNDP's work in Albania because Albania has a Gender Development Index of 0.967, indicating a high level of gender development in the country. UNDP partners with various organizations, including United Nations agencies and the United Nations Population Fund, to achieve its goals<a data-id='doc-13'>[1]</a>. UNDP also works with the government of Albania to drive policies and actions that focus on disaster risk reduction, increasing resilience, and protecting development investments<a data-id='doc-13'>[1]</a>. Additionally, UNDP focuses on the United Nations Sustainable Development Cooperation Framework in its work\"\n",
    "\n",
    "# Tokenize the texts\n",
    "references = [word_tokenize(ref) for ref in reference_texts]\n",
    "candidate = word_tokenize(candidate_text)\n",
    "\n",
    "# Calculate METEOR score\n",
    "score = meteor_score(references, candidate)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7917b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bert-score transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56474ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Your candidate (system outputs) and reference sentences\n",
    "candidates = [\"the cat sits on the mat\"]\n",
    "references = [\"the cat is sitting on the mat\"]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "\n",
    "# Print the precision, recall, and F1 scores\n",
    "for p, r, f1 in zip(P, R, F1):\n",
    "    print(f\"Precison: {p.item():.4f}, Recall: {r.item():.4f}, F1: {f1.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58cb31a",
   "metadata": {},
   "source": [
    "  # TODO::: \n",
    "  ##### Add citation prompt to the synthesis module. -done \n",
    "  ##### Convert notebook to flask API script. main.py - done\n",
    "  ##### Refactor PDF -> txt pipeline \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module to get information on the entities from user query using the KG\n",
    "def knowledgeGraphModule(user_query):\n",
    "    \n",
    "    # generate list of entities based on user query\n",
    "    entity_list = extractEntitiesFromQuery(user_query)\n",
    "    my_list = ast.literal_eval(entity_list)\n",
    "    prompt_summarise_entites = f\"\"\"\n",
    "    Summarize all relations between all the entities : {my_list}\n",
    "    \"\"\"\n",
    "    summarise_entities = callOpenAI(prompt_summarise_entites)\n",
    "    # Initialize an empty dictionary to store information\n",
    "    entities_dict = {\n",
    "        \"relations\": summarise_entities,\n",
    "        \"entities\": {}\n",
    "    }\n",
    "    # Loop through each entity in the list\n",
    "    for entity in my_list:\n",
    "        # Fetch information about the entity from your knowledge graph\n",
    "        prompt = f\"Give me a short description 50 words of {entity}\"\n",
    "        entity_info = callOpenAI(prompt)\n",
    "        # Add the entity information to the dictionary\n",
    "        entities_dict[\"entities\"][entity] = entity_info\n",
    "    \n",
    "    return entities_dict\n",
    "\n",
    "\n",
    "# Test usage\n",
    "test_query = \"what is the major work on UNDP in Afganistan?\"\n",
    "entities_dict = knowledgeGraphModule(test_query)\n",
    "print(entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationships_to_data_structure(relationships):\n",
    "    \"\"\"\n",
    "    Converts relationships from Neo4j query output into a structured Python data structure.\n",
    "\n",
    "    :param relationships: A list of Relationship objects from a Neo4j query.\n",
    "    :return: A list of dictionaries, each representing a relationship and its node details.\n",
    "    \"\"\"\n",
    "    formatted_relationships = []\n",
    "\n",
    "    # Sort the relationships by the name of the source node for consistent ordering\n",
    "    sorted_relationships = sorted(relationships, key=lambda rel: rel.start_node['name'])\n",
    "\n",
    "    for rel in sorted_relationships:\n",
    "        relationship_data = {\n",
    "            'relationship_type': rel.__class__.__name__,\n",
    "            'source_node': {\n",
    "                'name': rel.start_node['name'],\n",
    "                'category': rel.start_node.get('category', 'N/A'),\n",
    "                'summary': rel.start_node.get('summary', ''),\n",
    "                'acronym': rel.start_node.get('acronym', ''),\n",
    "                # Include additional properties as needed\n",
    "            },\n",
    "            'target_node': {\n",
    "                'name': rel.end_node['name'],\n",
    "                'category': rel.end_node.get('category', ''),\n",
    "                'summary': rel.end_node.get('summary', ''),\n",
    "                'acronym': rel.end_node.get('acronym', ''),\n",
    "\n",
    "                # Include additional properties as needed\n",
    "            },\n",
    "            'properties': {\n",
    "                # Include relationship properties\n",
    "                'description': rel.get('description', 'N/A'),\n",
    "            \n",
    "            }\n",
    "        }\n",
    "\n",
    "        formatted_relationships.append(relationship_data)\n",
    "\n",
    "    return formatted_relationships\n",
    "\n",
    "# Example usage, assuming 'relationships' is your list of Relationship objects from the query\n",
    "formatted_data = relationships_to_data_structure(entity_info['relationships'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_sort_relationships(relationships):\n",
    "    \"\"\"\n",
    "    Formats and sorts the relationships from Neo4j query output.\n",
    "\n",
    "    :param relationships: A list of Relationship objects from a Neo4j query.\n",
    "    \"\"\"\n",
    "    # Sort the relationships by the name of the source node for consistent ordering\n",
    "    sorted_relationships = sorted(relationships, key=lambda rel: rel.start_node['name'])\n",
    "\n",
    "    for rel in sorted_relationships:\n",
    "        source_node = rel.start_node\n",
    "        target_node = rel.end_node\n",
    "        print(f\"Relationship: {rel.__class__.__name__}\")\n",
    "        print(f\"  Source Node: {source_node['name']} (Category: {source_node.get('category', 'N/A')})\")\n",
    "        print(f\"  Target Node: {target_node['name']} (Category: {target_node.get('category', 'N/A')})\")\n",
    "        \n",
    "        # Print relationship properties with indentation\n",
    "        if rel.get('description'):\n",
    "            print(f\"  Description: {rel['description']}\")\n",
    "        if rel.get('summary'):\n",
    "            print(f\"  Summary: {rel['summary']}\")\n",
    "\n",
    "        print()  # Add a newline for spacing between relationships\n",
    "\n",
    "# Example usage, assuming 'relationships' is your list of Relationship objects from the query\n",
    "format_and_sort_relationships(entity_info['relationships'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
