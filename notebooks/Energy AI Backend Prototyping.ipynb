{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542a4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "from openai import AzureOpenAI\n",
    "import faiss\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf814d9",
   "metadata": {},
   "source": [
    "### Load Enviroment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4c742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f3e87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"api_key_azure\"),  \n",
    "  api_version = os.getenv(\"api_version\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "embedding_model = os.getenv(\"USER_QUERY_EMBEDDING_ENGINE\") \n",
    "\n",
    "# print(openai.api_key)\n",
    "# print(openai.api_base)\n",
    "# print(openai.api_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16534287",
   "metadata": {},
   "source": [
    "<h3>globals</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b2a3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query=\"What are the sustainable energy priorities for UNDP?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f11f01",
   "metadata": {},
   "source": [
    "<h3> helper functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc569537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to make simple openAI Calls\n",
    "def callOpenAI(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd62b3",
   "metadata": {},
   "source": [
    "<h3> processing modules </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96c95d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm instruction template here, with placeholders for insertion of user query, excerpts, indicator data, and entity and relation info\n"
     ]
    }
   ],
   "source": [
    "## module to create prompt engineering template  \n",
    "\n",
    "def promptEngineeringModule(user_query):\n",
    "    \n",
    "    ## generate prompt engineering text and template\n",
    "    llm_instructions=\"llm instruction template here, with placeholders for insertion of user query, excerpts, indicator data, and entity and relation info\" \n",
    "    \n",
    "    return llm_instructions\n",
    "\n",
    "# test usage\n",
    "llm_instructions=promptEngineeringModule(test_query)\n",
    "print(llm_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6798164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sustainable energy', 'UNDP']\n"
     ]
    }
   ],
   "source": [
    "def extractEntitiesFromQuery(user_query):\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities from the following user query: \\\"{user_query}\\\" and return output in array format e.g ['entity1','entity2']\n",
    "    \n",
    "    -Entities should be directly related to the domain or topic of interest. They should represent important concepts that contribute to the understanding of the subject matter.\n",
    "    -Each entity in the knowledge graph should be distinct and have a unique identifier. This ensures clarity and avoids ambiguity when establishing relationships between entities.\n",
    "    \"\"\"\n",
    "    entity_list = callOpenAI(prompt)   \n",
    "    return entity_list\n",
    "\n",
    "# Test usage\n",
    "test_query = \"What are the sustainable energy for UNDP?\"\n",
    "entity_list = extractEntitiesFromQuery(test_query)\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7718dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relations': 'The relation between UNDP and Afghanistan is that UNDP works in Afghanistan to support development projects and initiatives.', 'entities': {'UNDP': 'The United Nations Development Programme (UNDP) is a global organization that works to eradicate poverty, reduce inequalities, and promote sustainable development. It provides support to countries in areas such as governance, climate change, and crisis response, aiming to improve the lives of people and protect the planet.', 'Afghanistan': 'Afghanistan is a landlocked country located in Central Asia, bordered by Iran, Pakistan, Turkmenistan, Uzbekistan, Tajikistan, and China. It is known for its rugged mountainous terrain, rich cultural heritage, and complex history. Despite facing numerous challenges, Afghanistan is home to resilient people, diverse ethnic groups, and a vibrant blend of traditions.'}}\n"
     ]
    }
   ],
   "source": [
    "## module to get information on the entities from user query using the KG\n",
    "def knowledgeGraphModule(user_query):\n",
    "    \n",
    "    # generate list of entities based on user query\n",
    "    entity_list = extractEntitiesFromQuery(user_query)\n",
    "    my_list = ast.literal_eval(entity_list)\n",
    "    prompt_summarise_entites = f\"\"\"\n",
    "    Summarize all relations between all the entities : {my_list}\n",
    "    \"\"\"\n",
    "    summarise_entities = callOpenAI(prompt_summarise_entites)\n",
    "    # Initialize an empty dictionary to store information\n",
    "    entities_dict = {\n",
    "        \"relations\": summarise_entities,\n",
    "        \"entities\": {}\n",
    "    }\n",
    "    # Loop through each entity in the list\n",
    "    for entity in my_list:\n",
    "        # Fetch information about the entity from your knowledge graph\n",
    "        prompt = f\"Give me a short description 50 words of {entity}\"\n",
    "        entity_info = callOpenAI(prompt)\n",
    "        # Add the entity information to the dictionary\n",
    "        entities_dict[\"entities\"][entity] = entity_info\n",
    "    \n",
    "    return entities_dict\n",
    "\n",
    "\n",
    "# Test usage\n",
    "test_query = \"what is the major work on UNDP in Afganistan?\"\n",
    "entities_dict = knowledgeGraphModule(test_query)\n",
    "print(entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b85bd554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(user_query):\n",
    "    df = pd.read_pickle('../models/df_embed_EN.pkl')\n",
    "    df_filtered = df\n",
    "    length = len(df_filtered.head())\n",
    "    filtered_embeddings_arrays = np.array(list(df_filtered['Embedding']))\n",
    "    index = faiss.IndexFlatIP(filtered_embeddings_arrays.shape[1]) \n",
    "    index.add(filtered_embeddings_arrays)\n",
    "    \n",
    "    user_query_embedding = client.embeddings.create( \n",
    "        input=user_query ,model= embedding_model\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    k = min(5, length)\n",
    "    distances, indices = index.search(np.array([user_query_embedding]), k)\n",
    "    return df_filtered, distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c30f34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(user_question, content):\n",
    "    system_prompt = \"You are a system that answers user questions based on excerpts from PDF documents provided for context. Only answer if the answer can be found in the provided context. Do not make up the answer; if you cannot find the answer, say so.\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "        {'role': 'user', 'content': content},\n",
    "    ]\n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0.2,\n",
    "                    messages=messages\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27928260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_structure(qs):\n",
    "    result_dict = {}\n",
    "\n",
    "    # Extract the DataFrame from the tuple\n",
    "    dataframe = qs[0]\n",
    "\n",
    "    # Counter to limit the loop to 10 iterations\n",
    "    count = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Define a unique identifier for each document, you can customize this based on your data\n",
    "        document_id = f\"doc-{index + 1}\"\n",
    "        # Handle NaN in content by using fillna\n",
    "        content = row[\"Content\"]\n",
    "        content = ' '.join(row[\"Content\"].split()[:160])\n",
    "\n",
    "        # Create a dictionary for each document\n",
    "        document_info = {\n",
    "            \"title\": row[\"Document Title\"],\n",
    "            \"extract\": content or \"\",  # You may need to adjust this based on your column names\n",
    "            \"category\": row[\"Category\"],\n",
    "            \"link\": row[\"Link\"],\n",
    "            \"thumbnail\": ''\n",
    "        }\n",
    "        # print(document_info)\n",
    "        # Add the document to the result dictionary\n",
    "        result_dict[document_id] = document_info\n",
    "\n",
    "        # # Increment the counter\n",
    "        # count += 1\n",
    "\n",
    "        # # Break out of the loop if the counter reaches 10\n",
    "        # if count == 1:\n",
    "        #     break\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58c69dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpts saved to excerpts.json\n"
     ]
    }
   ],
   "source": [
    "## module to extract text from documents and return the text and document codes\n",
    "\n",
    "def semanticSearchModule(user_query):\n",
    "    qs = search_embeddings(user_query) #df, distances, indices\n",
    "    result_structure = map_to_structure(qs)\n",
    "    return result_structure\n",
    "\n",
    "#test usage\n",
    "excerpts_dict=semanticSearchModule(test_query)\n",
    "# print(excerpts_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Define the filename to save the JSON data -  can remove later\n",
    "json_filename = \"excerpts.json\"\n",
    "\n",
    "# Save excerpts_dict to the JSON file just for a better preview\n",
    "with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(excerpts_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Excerpts saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db75b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indicator-id-1': 'value from indicator-id-1', 'indicator-id-2': 'value from indicator-id-2'}\n"
     ]
    }
   ],
   "source": [
    "## module to get data for specific indicators which are identified is relevant to the user query\n",
    "\n",
    "def indicatorsModule(user_query): #lower priority\n",
    "    \n",
    "    # find relevant indicators based on uesr query and extract values\n",
    "    indicators_dict={\n",
    "        \"indicator-id-1\":\"value from indicator-id-1\",\n",
    "        \"indicator-id-2\":\"value from indicator-id-2\"\n",
    "    }#temp\n",
    "    \n",
    "    return indicators_dict\n",
    "\n",
    "#test usage\n",
    "indicators_dict=indicatorsModule(test_query)\n",
    "print(indicators_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8237482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt idea 1', 'prompt idea 2', 'prompt idea 3']\n"
     ]
    }
   ],
   "source": [
    "## module to generate query ideas\n",
    "\n",
    "def queryIdeationModule(user_query): # lower priority\n",
    "    \n",
    "    ## generate list of prompt ideas based on user query\n",
    "    query_idea_list=[\"prompt idea 1\",\"prompt idea 2\",\"prompt idea 3\"]#temp\n",
    "    \n",
    "    return query_idea_list\n",
    "\n",
    "#test usage\n",
    "query_idea_list=queryIdeationModule(test_query)\n",
    "print(query_idea_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b73a93",
   "metadata": {},
   "source": [
    "<h3> synthesis module </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7113366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to synthesize answer using retreival augmented generation approach\n",
    "\n",
    "def synthesisModule(user_query, llm_instructions, entities_dict, excerpts_dict, indicators_dict):\n",
    "    \n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer=\"test answer\"\n",
    "    \n",
    "    return answer  \n",
    "\n",
    "## to test this, run the full pipeline with the handleApiCall function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787b228",
   "metadata": {},
   "source": [
    "<h3> run pipeline </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10fe2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pipeline with retreival, synthesis of answer to user query, and structure results into api response\n",
    "\n",
    "def handleApiCall(user_query):\n",
    "        \n",
    "    ##run processing modules (in parallel)\n",
    "    llm_instructions=promptEngineeringModule(user_query)\n",
    "    entities_dict=knowledgeGraphModule(user_query)\n",
    "    excerpts_dict=semanticSearchModule(user_query)\n",
    "    indicators_dict=indicatorsModule(user_query) ##lower priority\n",
    "    query_idea_list=queryIdeationModule(user_query) ##lower priority\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer=synthesisModule(user_query, llm_instructions, entities_dict, excerpts_dict, indicators_dict)\n",
    "       \n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":excerpts_dict,\n",
    "        \"query_ideas\":query_idea_list,\n",
    "        \"entities\":list(entities_dict[\"entities\"].keys())       \n",
    "    }\n",
    "    \n",
    "    return response\n",
    "\n",
    "# test usage\n",
    "response=handleApiCall(test_query) \n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7cc6f",
   "metadata": {},
   "source": [
    "<h3>testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next step, develop automated testing for all modules\n",
    "## iterate through test_queries and build automated tests to score results\n",
    "\n",
    "# open testing dataset with queries and expected results\n",
    "test_queries_df=pd.read_csv(\"../testing/energy_ai_test_dataset_v0.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
