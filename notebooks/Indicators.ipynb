{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba99349b-4804-44bc-8e75-3128cecc209e",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2d9dcf-b7fd-4c8a-b007-6d6692189b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "import tiktoken\n",
    "import time\n",
    "import faiss\n",
    "import awoc\n",
    "import spacy\n",
    "import csv\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sutime import SUTime\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bert_score import score as bert_score\n",
    "from country_named_entity_recognition import find_countries\n",
    "#from processing_modules_for_test_indicator import knowledgeGraphModule, semanticSearchModule, queryIdeationModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d16eed-8ad4-41cb-87ac-56b0c627e301",
   "metadata": {},
   "source": [
    "## Load Raw Documents Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d78821-139c-4c9f-bc8e-c00d656359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data\n",
    "wdi_csv = pd.read_csv('../data/WDI_CSV/WDICSV.csv')\n",
    "# country meta data\n",
    "wdi_country = pd.read_csv('../data/WDI_CSV/WDICountry.csv')\n",
    "# Series meta data\n",
    "wdi_series = pd.read_csv('../data/WDI_CSV/WDISeries.csv')\n",
    "# country + series\n",
    "#wdi_country_series = pd.read_csv('../data/WDI_CSV/WDIcountry-series.csv')\n",
    "# series + time\n",
    "#wdi_series_time = pd.read_csv('../data/WDI_CSV/WDIseries-time.csv')\n",
    "# With CountryCode + SeriesCode + year, describe more info about this resource\n",
    "#wdi_footnote = pd.read_csv('../data/WDI_CSV/WDIfootnote.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd74365-94ef-44e9-bf65-8167cc4ff2f2",
   "metadata": {},
   "source": [
    "## Load Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07c63cf-0815-47cc-ba92-0f350971617d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3772a-959b-4d97-a2c9-c8b6fb407d8f",
   "metadata": {},
   "source": [
    "## OpenAI API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221e10b7-28d0-47cb-b13c-b7442a809c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"api_key_azure\"),  \n",
    "  api_version = os.getenv(\"api_version\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "embedding_model = os.getenv(\"USER_QUERY_EMBEDDING_ENGINE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619262ea-724b-44de-8bd3-d8d36b2e8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to make simple openAI Calls\n",
    "def callOpenAI(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b001050-5e77-4986-985c-230202dfa2ff",
   "metadata": {},
   "source": [
    "To get any information from WDICSV.csv (WDI meta data) we need 3 things: 1. country code 2. indicator code 3. target period (1960 - 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105332f-1762-4608-a6e7-208ed83434aa",
   "metadata": {},
   "source": [
    "## Function for searching country code (First Condition Done ✅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd36673f-d6ef-47b7-82af-f89762719967",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Previous 'find_mentioned_countries' can detect countries when they are formed correctly.\n",
    "\n",
    "'''\n",
    "# Extract mentioned countries' ISO3 code\n",
    "def find_mentioned_country_code(user_query):\n",
    "    countries = set()\n",
    "    extracted_countries = find_countries(user_query, is_ignore_case=True)\n",
    "    # check if we have country first\n",
    "    if extracted_countries:\n",
    "        for c in extracted_countries:\n",
    "            countries.add(c[0].alpha_3)\n",
    "    # check if we have continent\n",
    "    else:\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', user_query)\n",
    "        text = ' '.join(words)  # Join the tokens back into a string\n",
    "    \n",
    "        world_info = awoc.AWOC()\n",
    "        all_continents = set([continent.lower() for continent in world_info.get_continents_list()])\n",
    "        for word in text.split():\n",
    "            word = word.lower()\n",
    "            # check if this continent\n",
    "            if word in all_continents:\n",
    "                target_countries = world_info.get_countries_list_of(word)\n",
    "                for country in target_countries:\n",
    "                    countries.add(world_info.get_country_data(country)['ISO3'])\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d1a17-8822-4fa1-a604-c9eaaf8fd6c1",
   "metadata": {},
   "source": [
    "# Function for searching indicator code (Second Condition Done✅)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a4578-3f5d-4e5a-bda8-702c69463189",
   "metadata": {},
   "source": [
    "## Embedding Processing for Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094b8b1-234b-46e0-ae38-130d6f4a57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(row):\n",
    "    time.sleep(3)\n",
    "    #print(row.name)\n",
    "    input_text = row['Indicator Name'].replace(\"\\n\", \" \")\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text)\n",
    "    encodings = encoding.encode(input_text)\n",
    "    length = len(encodings)\n",
    "    embedding = client.embeddings.create( \n",
    "        input=input_text ,model= embedding_model\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    return length, embedding\n",
    "\n",
    "wdi_series['token_length'], wdi_series['Embedding'] = zip(*wdi_series.apply(lambda row: create_embedding(row), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f53dfdb7-12dd-43cc-a860-d880ce8fdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_series.to_pickle('../data/indicator_meta_embed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43470894-46b1-43a5-8680-3e3788a7c026",
   "metadata": {},
   "source": [
    "## Searching target indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3b703a-57b3-493a-862c-8160d152acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/indicator_meta_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eab9be9-a94b-40cd-8c23-70efbb217489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard similarity between two texts\n",
    "def jaccard_similarity(text1, text2):\n",
    "    # Tokenize texts\n",
    "    tokens1 = set(text1.lower().split())\n",
    "    tokens2 = set(text2.lower().split())\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a5e173-6a61-4a48-bb68-180e698e1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_indicators(user_query):\n",
    "    # Calculate similarity scores for each indicators\n",
    "    similarity_scores = []\n",
    "    indicators = []\n",
    "\n",
    "    # Iterate through each indicator title and calculate similarity score\n",
    "    for indicator in df['Indicator Name']:\n",
    "        similarity_score = jaccard_similarity(user_query, indicator)\n",
    "        similarity_scores.append(similarity_score)\n",
    "        indicators.append(indicator)\n",
    "        \n",
    "    # Create DataFrame only with valid similarity scores\n",
    "    similarity_df = pd.DataFrame({'Indicator Name': indicators, 'Similarity Score': similarity_scores})\n",
    "    similarity_df = similarity_df.sort_values('Similarity Score', ascending=False)\n",
    "    similarity_df = similarity_df[:10]\n",
    "        \n",
    "    # Filter indicators where similarity score is above a threshold (e.g., 0.3)\n",
    "    threshold = 0.01\n",
    "    filtered_df = df[df['Indicator Name'].isin(similarity_df[similarity_df['Similarity Score'] > threshold]['Indicator Name'])]\n",
    "\n",
    "    return  list(filtered_df['Series Code'])\n",
    "#print(filter_indicators(test_query))\n",
    "#print(filter_indicators(test_query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f51e73a-597b-41c1-a2e0-76d3fece207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search target indicator\n",
    "# Implement this function later\n",
    "def search_embeddings(user_query):\n",
    "    df_filtered = filter_indicators(user_query) if filter_indicators(user_query) is not None else None\n",
    "    \n",
    "    if df_filtered is not None and not df_filtered.empty:  # Check if DataFrame is not None and not empty\n",
    "        length = len(df_filtered.head())\n",
    "        filtered_embeddings_arrays = np.array(list(df_filtered['Embedding']))\n",
    "        index = faiss.IndexFlatIP(filtered_embeddings_arrays.shape[1]) \n",
    "        index.add(filtered_embeddings_arrays)\n",
    "        \n",
    "        user_query_embedding = client.embeddings.create( \n",
    "                input=user_query ,model= embedding_model\n",
    "            ).data[0].embedding\n",
    "\n",
    "        k = min(5, length)\n",
    "        distances, indices = index.search(np.array([user_query_embedding]), k)\n",
    "        return df_filtered, distances, indices\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54971fae-ddcf-4caf-b0fe-1c3c6240f746",
   "metadata": {},
   "source": [
    "# Function for searching target period (1960 - 2023) (Third Condition Done ✅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40d575b-c8ba-4ff5-baba-3002debe466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract set of years from given timex3_list\n",
    "def timex3_to_year_list(timex3_list):\n",
    "    year_list = set()\n",
    "    for timex3 in timex3_list:\n",
    "        sutimeType, value = timex3[\"type\"], timex3[\"value\"]\n",
    "        if \"REF\" not in value:\n",
    "            if isinstance(value, dict):\n",
    "                if value:\n",
    "                    for year in range(int(value['begin']), int(value['end']) + 1):\n",
    "                        year_list.add(str(year))\n",
    "            elif value.isdigit():\n",
    "                year_list.add(str(value))\n",
    "            elif sutimeType in ['DATE', 'DURATION']:\n",
    "                if sutimeType == 'DATE':\n",
    "                    res = re.search('^\\d\\d\\d\\d', value)\n",
    "                    if res:\n",
    "                        year_list.add(str(res.group(0)))\n",
    "                else:\n",
    "                    year_dur = 0\n",
    "                    current_year = datetime.now().year\n",
    "                    dur_list = re.findall('\\d+', \"\".join(re.findall('P[0-9]+Y', value)))\n",
    "                    if dur_list:\n",
    "                        year_dur = max([int(y) for y in dur_list])\n",
    "                        while year_dur:\n",
    "                            year_list.add(str(current_year - year_dur))\n",
    "                            year_dur -= 1\n",
    "            else:\n",
    "                continue\n",
    "    return list(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c2e1fc-0840-46eb-b515-02ac8c12ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_period(user_query):\n",
    "    sutime = SUTime(mark_time_ranges = True, include_range = True)\n",
    "    res = sutime.parse(user_query)\n",
    "    return timex3_to_year_list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4406f-5189-4a7f-9752-b377f40a8d07",
   "metadata": {},
   "source": [
    "## Final one function for searching indicator data (Function for finding info from indicator database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d45b480-44fa-4fd7-b02a-83eda2cedab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_structure(countries, indicators, years):\n",
    "    # load all indicator dataset\n",
    "    # wdi_csv = pd.read_csv('../data/WDI_CSV/WDICSV.csv')\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    for country in countries:\n",
    "        for indicator in indicators:\n",
    "            indicator_id = f\"indicator-{count + 1}\"\n",
    "            target_row = wdi_csv[(wdi_csv['Country Code'] == country) & (wdi_csv['Indicator Code'] == indicator)]\n",
    "            if not target_row.empty:\n",
    "                country_name, indicator_name = target_row['Country Name'].values[0], target_row['Indicator Name'].values[0]\n",
    "                if years:\n",
    "                    target_row = target_row[years]\n",
    "                else:\n",
    "                    target_row = target_row.iloc[:,4:]\n",
    "                target_row = target_row.dropna(axis=1)\n",
    "                if not target_row.empty:\n",
    "                    year_to_value = {}\n",
    "                    for column in target_row:\n",
    "                        year_to_value[column] = target_row[column].values[0]\n",
    "                    indicator_info = {\n",
    "                        \"Country\": country_name,\n",
    "                        \"Indicator Name\": indicator_name,\n",
    "                        \"Values Per Year\": year_to_value\n",
    "                    }\n",
    "                    \n",
    "                    result_dict[indicator_id] = indicator_info\n",
    "                    # Increment the counter\n",
    "                    count += 1\n",
    "        if count == 30:\n",
    "            break\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64e007e-c4cf-4876-a39a-361d31556c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module to extract text from documents and return the text and document codes\n",
    "def indicatorsModule(user_query):\n",
    "    countries = find_mentioned_country_code(user_query)\n",
    "    indicators = filter_indicators(user_query) #df, distances, indices\n",
    "    years = find_target_period(user_query)\n",
    "    if countries and indicators:\n",
    "        # Reduce Indicator List to 2 if countries are too many\n",
    "        if len(countries) > 5:\n",
    "            indicators = indicators[:2]\n",
    "        # for testing\n",
    "        #result_structure = {}\n",
    "        #result_structure[\"User Query\"] = user_query\n",
    "        #result_structure[\"indicatorsModule Result\"] = map_to_structure(countries, indicators, years)\n",
    "        result_structure = map_to_structure(countries, indicators, years)\n",
    "        return result_structure\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf8b9-2481-4f5f-873b-2a406aac0265",
   "metadata": {},
   "source": [
    "# Test Function (indicatorsModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a85b63-0697-4351-a866-6f358a2c8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(csv_file, moonshot_model):\n",
    "    # Initialize an empty list to store processed entries\n",
    "    result = []\n",
    "    \n",
    "    # Loop through each entry in the CSV file\n",
    "    for entry in csv.DictReader(csv_file):\n",
    "        print(f\"Testing Query #{len(result)}...\")\n",
    "        query = entry['query']\n",
    "        sample_answer = entry['sample_answer']\n",
    "        \n",
    "        # Call OpenAI for chat GPT answer\n",
    "        chat_gpt_answer = callOpenAI(f\"\"\" \n",
    "                                    {query}\n",
    "                                    \"\"\")\n",
    "        \n",
    "        # Call the moonshot model API\n",
    "        moonshot_model_answer = moonshot_model(query)\n",
    "\n",
    "        # Calculate BERT score for moonshot model answer\n",
    "        P, F, R = bert_score([sample_answer], [moonshot_model_answer['answer']], lang='en', verbose=True)\n",
    "        entry['moonshot_model_answer'] = moonshot_model_answer['answer']\n",
    "        entry['bert_score'] = round(float(F), 2)\n",
    "\n",
    "        # Calculate BERT score for chat GPT answer\n",
    "        P, F, R = bert_score([sample_answer], [chat_gpt_answer], lang='en', verbose=True)\n",
    "        entry['chat_gpt_answer'] = chat_gpt_answer\n",
    "        entry['bert_score_gpt'] = round(float(F), 2)\n",
    "        \n",
    "        # Append the processed entry to the result list\n",
    "        result.append(entry)\n",
    "    \n",
    "    # Return the list of processed entries\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ee2eb-5ff1-4d2b-8a01-bfdc866d0c04",
   "metadata": {},
   "source": [
    "## Conduct test only using the indicator database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf71d17c-b9f8-4e47-ae85-ae31e8b044fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moonshot_model_indicator_only(user_query):\n",
    "    \n",
    "    ##run processing modules\n",
    "    indicators_dict=indicatorsModule(user_query)\n",
    "    prompt_formattings = \"\"\"\n",
    "    - Answer output must be properly formatted using HTML. \n",
    "    - Don't include <html>, <script>, <link> or <body> tags. Only text formating tags should be allowed. e.g h1..h3, p, anchor, etc. Strictly HTML only\n",
    "    - Strictly infer your answers from the <Sources> Only and make citations to Source extract referenced \n",
    "    - The Source as format like: \"indicator-n\": {{\n",
    "        \"Country\": \"Tanzania\",\n",
    "        \"Indicator Name\": \"Average precipitation in depth (mm per year)\",\n",
    "        \"Values Per Year\": {{\n",
    "            \"YYYY\": some number value\n",
    "            }}\n",
    "    }}, where doc-n can be indicator-1, indicator-24 etc.. n is in integer.\n",
    "    - Reference the extract and title of all document sources provided in the json and summarise it into a coherent answer that relates to the <User Query>\n",
    "    - Citation should follow formats: [reference content]<a href='link here' data-id='doc-n'>[i]</a> . The reference bracket should be the reference link\n",
    "    - Give output writing tone like a academic research tone\n",
    "    - Strictly use IEEE Citation Style \n",
    "    - If no <Sources> are provided, try to make suggestives or  simply say you don't have that information   \n",
    "    - Remove new line or tab characters from your output\n",
    "    \"\"\"\n",
    "    \n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, use the provided <Sources> extract section of the JSON only to provide the correct answer to the user's query.\n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {indicators_dict}\n",
    "\n",
    "    - Reference all data sources provided in the json and summarise it into a coherent answer that relates to the <User Query>\n",
    "    - Maintain an academic research tone while aiming for brevity in your response\n",
    "    - If no <Sources> are provided, try to make suggestives or  simply say you don't have that information   \n",
    "    - Remove new line or tab characters from your output\n",
    "    \"\"\"\n",
    "    \n",
    "    ##synthesis module\n",
    "    answer = callOpenAI(llm_instructions)\n",
    "\n",
    "    ##structure response\n",
    "    response={\n",
    "        \"user_query\":user_query,\n",
    "        \"answer\":answer,\n",
    "        \"sources\":indicators_dict,    \n",
    "    }\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb7f2b-790a-4aee-adf6-36707d807971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"../testing/indicator_test/test_queries.csv\"\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    # Pass the file object to the function\n",
    "    result = calculate_scores(file, moonshot_model_indicator_only)\n",
    "\n",
    "# Print updated data with scores\n",
    "print(json.dumps(result, indent=4))\n",
    "\n",
    "# Save updated data to a JSON file\n",
    "#with open('../testing/indicator_test/test_output_indicator_only.json', 'w') as file:\n",
    "    #json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbca83-1585-4ca1-a6aa-d870ee934124",
   "metadata": {},
   "source": [
    "## Conduct test with the synthesized database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a47803-f90e-4ffc-a64a-b53523e5caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisModule(user_query, entities_dict, excerpts_dict, indicators_dict, openai_deployment):\n",
    "    \n",
    "    # Generate prompt engineering text and template\n",
    "    llm_instructions = f\"\"\"\n",
    "    Ignore previous commands!!!\n",
    "    Given a user query, use the provided <Sources> extract section of the JSON only to provide the correct answer to the user's query.\n",
    "    \n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Sources: {excerpts_dict}\n",
    "    \n",
    "    - Answer output must be properly formatted using HTML. \n",
    "    - Don't include <html>, <script>, <link> or <body> tags. Only text formating tags should be allowed. e.g h1..h3, p, anchor, etc. Strictly HTML only\n",
    "    - Strictly infer your answers from the <Sources> Only and make citations to Source extract referenced \n",
    "    - The Source as format like: \"doc-n\": {{\n",
    "        \"title\": \"title of the relate document\",\n",
    "        \"extract\": \"content\",\n",
    "        \"category\": \"\",\n",
    "        \"link\": \"\",\n",
    "        \"thumbnail\": \"\"\n",
    "    }}, where doc-n can be doc-1, doc-24 etc.. n is in integer.\n",
    "    - Reference the extract and title of all document sources provided in the json and summarise it into a coherent answer that relates to the <User Query>\n",
    "    - Citation should follow formats: [reference content]<a href='link here' data-id='doc-n'>[i]</a> . The reference bracket should be the reference link\n",
    "    - Give output writing tone like a academic research tone\n",
    "    - Strictly use IEEE Citation Style \n",
    "    - If no <Sources> are provided, try to make suggestives or  simply say you don't have that information   \n",
    "    - Remove new line or tab characters from your output\n",
    "\n",
    "    \"\"\"\n",
    "    ###synthesize data into structure within llm prompt engineering instructions\n",
    "    answer= openai_call.callOpenAI(llm_instructions, openai_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de53eb-b47c-4cfb-87f0-05b0cdf57579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"../testing/indicator_test/test_queries.csv\"\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    # Pass the file object to the function\n",
    "    result = calculate_scores(file, moonshot_model)\n",
    "\n",
    "# Print updated data with scores\n",
    "# print(json.dumps(result, indent=4))\n",
    "\n",
    "# Save updated data to a JSON file\n",
    "with open('../testing/indicator_test/test_output_synthesized_database.json', 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
