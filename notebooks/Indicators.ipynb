{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba99349b-4804-44bc-8e75-3128cecc209e",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2d9dcf-b7fd-4c8a-b007-6d6692189b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "import tiktoken\n",
    "import time\n",
    "import faiss\n",
    "import awoc\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sutime import SUTime\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d16eed-8ad4-41cb-87ac-56b0c627e301",
   "metadata": {},
   "source": [
    "## Load Raw Documents Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d78821-139c-4c9f-bc8e-c00d656359a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data\n",
    "wdi_csv = pd.read_csv('../data/WDI_CSV/WDICSV.csv')\n",
    "# country meta data\n",
    "wdi_country = pd.read_csv('../data/WDI_CSV/WDICountry.csv')\n",
    "# Series meta data\n",
    "wdi_series = pd.read_csv('../data/WDI_CSV/WDISeries.csv')\n",
    "# country + series\n",
    "#wdi_country_series = pd.read_csv('../data/WDI_CSV/WDIcountry-series.csv')\n",
    "# series + time\n",
    "#wdi_series_time = pd.read_csv('../data/WDI_CSV/WDIseries-time.csv')\n",
    "# With CountryCode + SeriesCode + year, describe more info about this resource\n",
    "#wdi_footnote = pd.read_csv('../data/WDI_CSV/WDIfootnote.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd74365-94ef-44e9-bf65-8167cc4ff2f2",
   "metadata": {},
   "source": [
    "## Load Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07c63cf-0815-47cc-ba92-0f350971617d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3772a-959b-4d97-a2c9-c8b6fb407d8f",
   "metadata": {},
   "source": [
    "## OpenAI API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221e10b7-28d0-47cb-b13c-b7442a809c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv(\"api_key_azure\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"api_version\")\n",
    "openai_deployment = \"sdgi-gpt-35-turbo-16k\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"api_key_azure\"),  \n",
    "  api_version = os.getenv(\"api_version\"),\n",
    "  azure_endpoint =os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    ")\n",
    "\n",
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "embedding_model = os.getenv(\"USER_QUERY_EMBEDDING_ENGINE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619262ea-724b-44de-8bd3-d8d36b2e8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to make simple openAI Calls\n",
    "def callOpenAI(prompt):  \n",
    "    response_entities = openai.chat.completions.create(\n",
    "                    model=openai_deployment,\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                )\n",
    "    response = response_entities.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b001050-5e77-4986-985c-230202dfa2ff",
   "metadata": {},
   "source": [
    "To get any information from WDICSV.csv (WDI meta data) we need 3 things: 1. country code 2. indicator code 3. target period (1960 - 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105332f-1762-4608-a6e7-208ed83434aa",
   "metadata": {},
   "source": [
    "## Function for searching country code (First Condition Done ✅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd36673f-d6ef-47b7-82af-f89762719967",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Previous 'find_mentioned_countries' cannot catch continent -> Modify a bit\n",
    "'''\n",
    "# Extract mentioned countries' ISO3 code\n",
    "def find_mentioned_country_code(user_query):\n",
    "    countries = set()\n",
    "    \n",
    "    # Tokenize the text using regular expressions to preserve punctuation marks\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', user_query)\n",
    "    text = ' '.join(words)  # Join the tokens back into a string\n",
    "    \n",
    "    world_info = awoc.AWOC()\n",
    "\n",
    "    all_continents = set([continent.lower() for continent in world_info.get_continents_list()])\n",
    "    all_countries = set([country.lower() for country in world_info.get_countries_list()])\n",
    "    \n",
    "    for word in text.split():\n",
    "        word = word.lower()\n",
    "        # check if this continent\n",
    "        if word in all_countries:\n",
    "            countries.add(world_info.get_country_data(word)['ISO3'])\n",
    "        elif word in all_continents:\n",
    "            target_countries = world_info.get_countries_list_of(word)\n",
    "            for country in target_countries:\n",
    "                countries.add(world_info.get_country_data(country)['ISO3'])\n",
    "    return countries\n",
    "#print(find_mentioned_country_code(test_query))\n",
    "#print(find_mentioned_country_code(test_query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d1a17-8822-4fa1-a604-c9eaaf8fd6c1",
   "metadata": {},
   "source": [
    "# Function for searching indicator code (Second Condition Done✅)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a4578-3f5d-4e5a-bda8-702c69463189",
   "metadata": {},
   "source": [
    "## Embedding Processing for Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5094b8b1-234b-46e0-ae38-130d6f4a57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(row):\n",
    "    time.sleep(3)\n",
    "    #print(row.name)\n",
    "    input_text = row['Indicator Name'].replace(\"\\n\", \" \")\n",
    "    input_text = re.sub(r'\\s+', ' ', input_text)\n",
    "    encodings = encoding.encode(input_text)\n",
    "    length = len(encodings)\n",
    "    embedding = client.embeddings.create( \n",
    "        input=input_text ,model= embedding_model\n",
    "    ).data[0].embedding\n",
    "    \n",
    "    return length, embedding\n",
    "\n",
    "wdi_series['token_length'], wdi_series['Embedding'] = zip(*wdi_series.apply(lambda row: create_embedding(row), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f53dfdb7-12dd-43cc-a860-d880ce8fdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_series.to_pickle('../data/indicator_meta_embed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43470894-46b1-43a5-8680-3e3788a7c026",
   "metadata": {},
   "source": [
    "## Searching target indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e3b703a-57b3-493a-862c-8160d152acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/indicator_meta_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eab9be9-a94b-40cd-8c23-70efbb217489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard similarity between two texts\n",
    "def jaccard_similarity(text1, text2):\n",
    "    # Tokenize texts\n",
    "    tokens1 = set(text1.lower().split())\n",
    "    tokens2 = set(text2.lower().split())\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a5e173-6a61-4a48-bb68-180e698e1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_indicators(user_query):\n",
    "    # Calculate similarity scores for each indicators\n",
    "    similarity_scores = []\n",
    "    indicators = []\n",
    "\n",
    "    # Iterate through each indicator title and calculate similarity score\n",
    "    for indicator in df['Indicator Name']:\n",
    "        similarity_score = jaccard_similarity(user_query, indicator)\n",
    "        similarity_scores.append(similarity_score)\n",
    "        indicators.append(indicator)\n",
    "        \n",
    "    # Create DataFrame only with valid similarity scores\n",
    "    similarity_df = pd.DataFrame({'Indicator Name': indicators, 'Similarity Score': similarity_scores})\n",
    "    similarity_df = similarity_df.sort_values('Similarity Score', ascending=False)\n",
    "    similarity_df = similarity_df[:10]\n",
    "        \n",
    "    # Filter indicators where similarity score is above a threshold (e.g., 0.3)\n",
    "    threshold = 0.01\n",
    "    filtered_df = df[df['Indicator Name'].isin(similarity_df[similarity_df['Similarity Score'] > threshold]['Indicator Name'])]\n",
    "\n",
    "    return  list(filtered_df['Series Code'])\n",
    "#print(filter_indicators(test_query))\n",
    "#print(filter_indicators(test_query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f51e73a-597b-41c1-a2e0-76d3fece207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search target indicator\n",
    "# Implement this function later\n",
    "def search_embeddings(user_query):\n",
    "    df_filtered = filter_indicators(user_query) if filter_indicators(user_query) is not None else None\n",
    "    \n",
    "    if df_filtered is not None and not df_filtered.empty:  # Check if DataFrame is not None and not empty\n",
    "        length = len(df_filtered.head())\n",
    "        filtered_embeddings_arrays = np.array(list(df_filtered['Embedding']))\n",
    "        index = faiss.IndexFlatIP(filtered_embeddings_arrays.shape[1]) \n",
    "        index.add(filtered_embeddings_arrays)\n",
    "        \n",
    "        user_query_embedding = client.embeddings.create( \n",
    "                input=user_query ,model= embedding_model\n",
    "            ).data[0].embedding\n",
    "\n",
    "        k = min(5, length)\n",
    "        distances, indices = index.search(np.array([user_query_embedding]), k)\n",
    "        return df_filtered, distances, indices\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54971fae-ddcf-4caf-b0fe-1c3c6240f746",
   "metadata": {},
   "source": [
    "# Function for searching target period (1960 - 2023) (Third Condition Done ✅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a40d575b-c8ba-4ff5-baba-3002debe466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract set of years from given timex3_list\n",
    "def timex3_to_year_list(timex3_list):\n",
    "    year_list = set()\n",
    "    for timex3 in timex3_list:\n",
    "        sutimeType, value = timex3[\"type\"], timex3[\"value\"]\n",
    "        if \"REF\" not in value:\n",
    "            if isinstance(value, dict):\n",
    "                for year in range(int(value['begin']), int(value['end']) + 1):\n",
    "                    year_list.add(str(year))\n",
    "            elif value.isdigit():\n",
    "                year_list.add(str(value))\n",
    "            elif sutimeType in ['DATE', 'DURATION']:\n",
    "                if sutimeType == 'DATE':\n",
    "                    res = re.search('^\\d\\d\\d\\d', value)\n",
    "                    if res:\n",
    "                        year_list.add(str(res.group(0)))\n",
    "                else:\n",
    "                    year_dur = 0\n",
    "                    current_year = datetime.now().year\n",
    "                    dur_list = re.findall('\\d+', \"\".join(re.findall('P[0-9]+Y', value)))\n",
    "                    if dur_list:\n",
    "                        year_dur = max([int(y) for y in dur_list])\n",
    "                        while year_dur:\n",
    "                            year_list.add(str(current_year - year_dur))\n",
    "                            year_dur -= 1\n",
    "            else:\n",
    "                continue\n",
    "    return list(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c2e1fc-0840-46eb-b515-02ac8c12ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"How many people in Afghanistan lack access to energy/electricity/clean cooking solutions for past 3 years?\"\n",
    "def find_target_period(user_query):\n",
    "    sutime = SUTime(mark_time_ranges = True, include_range = True)\n",
    "    res = sutime.parse(user_query)\n",
    "    return timex3_to_year_list(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4406f-5189-4a7f-9752-b377f40a8d07",
   "metadata": {},
   "source": [
    "## Final one function for searching indicator data (Function for finding info from indicator database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d45b480-44fa-4fd7-b02a-83eda2cedab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_structure(countries, indicators, years):\n",
    "    # load all indicator dataset\n",
    "    # wdi_csv = pd.read_csv('../data/WDI_CSV/WDICSV.csv')\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    for country in countries:\n",
    "        for indicator in indicators:\n",
    "            indicator_id = f\"wdi-{count + 1}\"\n",
    "            target_row = wdi_csv[(wdi_csv['Country Code'] == country) & (wdi_csv['Indicator Code'] == indicator)]\n",
    "            if not target_row.empty:\n",
    "                country_name, indicator_name = target_row['Country Name'].values[0], target_row['Indicator Name'].values[0]\n",
    "                if years:\n",
    "                    target_row = target_row[years]\n",
    "                else:\n",
    "                    target_row = target_row.iloc[:,4:]\n",
    "                target_row = target_row.dropna(axis=1)\n",
    "                if not target_row.empty:\n",
    "                    year_to_value = {}\n",
    "                    for column in target_row:\n",
    "                        year_to_value[column] = target_row[column].values[0]\n",
    "                    indicator_info = {\n",
    "                        \"Country\": country_name,\n",
    "                        \"Indicator Name\": indicator_name,\n",
    "                        \"Values Per Year\": year_to_value\n",
    "                    }\n",
    "                    \n",
    "                    result_dict[indicator_id] = indicator_info\n",
    "                    # Increment the counter\n",
    "                    count += 1\n",
    "        if count == 30:\n",
    "            break\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e64e007e-c4cf-4876-a39a-361d31556c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## module to extract text from documents and return the text and document codes\n",
    "def indicatorsModule(user_query):\n",
    "    countries = find_mentioned_country_code(user_query)\n",
    "    indicators = filter_indicators(user_query) #df, distances, indices\n",
    "    years = find_target_period(user_query)\n",
    "    if countries and indicators:\n",
    "        # Reduce Indicator List to 2 if countries are too many\n",
    "        if len(countries) > 5:\n",
    "            indicators = indicators[:2]\n",
    "        result_structure = map_to_structure(countries, indicators, years)\n",
    "        return result_structure\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf8b9-2481-4f5f-873b-2a406aac0265",
   "metadata": {},
   "source": [
    "# Test Function (indicatorsModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e42ecfa-2a94-48a6-9690-4bd0e282614a",
   "metadata": {},
   "source": [
    "## User Query Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cf7abef-b544-4525-8465-2e567f29a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query1 = \"How many people in Afghanistan lack access to energy/electricity/clean cooking solutions for past 3 years?\"\n",
    "test_query2 = \"How much is invested in energy in Africa per annum from 2020 to 2023?\"\n",
    "test_query3 = \"What is the social and economic impact of investing 1 million USD in clean energy in albania?\"\n",
    "test_query4 = \"What is the socio-economic and environmental impact of the rising demand for critical minerals on indigenous communities in india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c84ce8c9-26e0-4278-827b-25eba8662170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query # 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Registering annotator sutime with class edu.stanford.nlp.time.TimeAnnotator\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sutime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Registering annotator sutime with class edu.stanford.nlp.time.TimeAnnotator\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sutime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query # 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Registering annotator sutime with class edu.stanford.nlp.time.TimeAnnotator\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sutime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query # 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Registering annotator sutime with class edu.stanford.nlp.time.TimeAnnotator\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sutime\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i, test_query in enumerate([test_query1, test_query2, test_query3, test_query4]):\n",
    "    print(f\"Query # {i}\")\n",
    "    indicator_data = indicatorsModule(test_query)\n",
    "    result.append(indicator_data)\n",
    "with open('../testing/indicatorModule_test_output.json', 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
